{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8674462,"sourceType":"datasetVersion","datasetId":5199276}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os, sys\nimport pandas as pd\nimport shutil\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-16T19:41:23.035423Z","iopub.execute_input":"2024-06-16T19:41:23.036332Z","iopub.status.idle":"2024-06-16T19:41:44.323532Z","shell.execute_reply.started":"2024-06-16T19:41:23.036289Z","shell.execute_reply":"2024-06-16T19:41:44.322633Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-16 19:41:28.807618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-16 19:41:28.807739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-16 19:41:29.049566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_frames(mode = \"train\"):\n    \"\"\"Extracts frames from the video\n    NOTE: The function checks if directory exists or not,\n    if the directory already exists, it does not extract \n    the frames. Be sure that there is no already existing directory\n    and function let the function entirely.\"\"\"\n    dir_path = os.path.join(\"./data\", mode+'_imgs')\n    if not os.path.isdir(dir_path):\n        os.makedirs(dir_path)\n        vidcap = cv2.VideoCapture('./data/{}.mp4'.format(mode))\n        success,image = vidcap.read()\n        count = 0\n        while success:\n            cv2.imwrite(\"%s/%d.jpg\" % (dir_path,count), image)     # save frame as JPEG file      \n            success,image = vidcap.read()\n            count += 1\n            if count % 1000 == 0:\n                print(\"%d frames read\" % count)\n    else:\n        print(\"Video frames already extracted \\nSkipping extraction...\")\n\n    frame_cnt = len(os.listdir(dir_path))\n    vid_frames = np.empty((frame_cnt,480,640,3),dtype='uint8')\n    for i in range(0,frame_cnt):\n                frame = cv2.imread(dir_path + '/' + str(i) + \".jpg\")\n                # print(\",\",frame.shape)\n                vid_frames[i] = frame\n                sys.stdout.write(\"\\rLoading frame \" + str(i))\n    return vid_frames\nload_frames()","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:06:57.985861Z","iopub.execute_input":"2024-06-16T12:06:57.986480Z","iopub.status.idle":"2024-06-16T12:06:58.023855Z","shell.execute_reply.started":"2024-06-16T12:06:57.986450Z","shell.execute_reply":"2024-06-16T12:06:58.022916Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"array([], shape=(0, 480, 640, 3), dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"# class OptFlowNet:\n\n#     def __init__(self, WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE):\n#         self.WINSIZE = WINSIZE\n#         self.WEIGHTS = WEIGHTS\n#         self.EPOCHS = EPOCHS\n#         self.BATCH_SIZE = BATCH_SIZE\n#         self.optflow_dir = \"optical_flow_data\"  # Directory to save optical flow images\n\n#     def process_frame(self, frame):\n#         frame = cv2.resize(frame, (self.WINSIZE[1], self.WINSIZE[0]), interpolation=cv2.INTER_AREA)\n#         frame = frame / 127.5 - 1.0  # standardize the image to mean = 0 and std = 1\n#         return frame\n\n#     def optflow(self, frame1, frame2):\n#         frame1 = frame1[200:400]\n#         frame1 = cv2.resize(frame1, (0, 0), fx=0.4, fy=0.5)\n#         frame2 = frame2[200:400]\n#         frame2 = cv2.resize(frame2, (0, 0), fx=0.4, fy=0.5)\n#         flow = np.zeros_like(frame1)\n#         prev = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n#         nxt = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n#         flow_data = cv2.calcOpticalFlowFarneback(prev, nxt, None, 0.4, 1, 12, 2, 8, 1.2, 0)\n#         mag, ang = cv2.cartToPolar(flow_data[..., 0], flow_data[..., 1])\n#         flow[..., 1] = 255  # saturation\n#         flow[..., 0] = ang * 180 / np.pi / 2  # hue\n#         flow[..., 2] = (mag * 15).astype(int)  # value\n#         return flow\n\n#     def prep_data(self, video_file, label_file, shuffle=False, wipe=False, mode=\"train\"):\n#         print(\"Decoding Labels...\")\n#         if mode != \"test\":\n#             speed_data = np.array(pd.read_csv(label_file, header=None))\n#             speed_data = speed_data.flatten()[:-1]  # assuming the last entry is redundant\n#             print(f\"Loaded {len(speed_data)} labels\")\n#         else:\n#             shuffle = False\n#             speed_data = np.array([])\n\n#         if wipe and os.path.isdir(self.optflow_dir):\n#             print(\"Wiping preprocessed data...\")\n#             shutil.rmtree(self.optflow_dir)\n\n#         processed_video = None\n#         if not os.path.isdir(self.optflow_dir):\n#             print(\"Preprocessing video...\")\n#             os.mkdir(self.optflow_dir)\n#             vid = cv2.VideoCapture(video_file)\n#             frame_cnt = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n#             processed_video = np.empty((frame_cnt - 1, self.WINSIZE[0], self.WINSIZE[1], 3), dtype='uint8')\n#             success, prev = vid.read()\n#             i = 0\n#             while True:\n#                 success, nxt = vid.read()\n#                 if not success:\n#                     break\n#                 flow = self.optflow(prev, nxt)\n#                 prev = nxt\n#                 flow = cv2.resize(flow, self.WINSIZE, interpolation=cv2.INTER_AREA)\n#                 processed_video[i] = flow / 127.5 - 1.0\n#                 cv2.imwrite(f\"{self.optflow_dir}/{i}.png\", flow)\n#                 sys.stdout.write(f\"\\rProcessed {i} frames\")\n#                 i += 1\n#             print(f\"\\nDone processing {frame_cnt} frames\")\n#         else:\n#             print(\"Found preprocessed data\")\n#             frame_cnt = len(os.listdir(self.optflow_dir))\n#             processed_video = np.empty((frame_cnt, self.WINSIZE[0], self.WINSIZE[1], 3), dtype='float32')\n#             for i in range(frame_cnt):\n#                 flow = cv2.imread(f\"{self.optflow_dir}/{i}.png\")\n#                 flow = self.process_frame(flow)\n#                 processed_video[i] = flow\n#                 sys.stdout.write(f\"\\rLoading frame {i}\")\n#             print(f\"\\nDone loading {frame_cnt} frames\")\n\n#         if shuffle:\n#             print(\"Shuffling data\")\n#             randomize = np.arange(len(processed_video))\n#             np.random.shuffle(randomize)\n#             processed_video = processed_video[randomize]\n#             speed_data = speed_data[randomize]\n\n#         print(\"Done prepping data\")\n#         return processed_video, speed_data\n\n#     def create_model(self):\n#         print(\"Compiling Model...\")\n#         self.model = Sequential()\n#         self.model.add(Conv2D(32, (8, 8), padding='same', strides=(4, 4), input_shape=(self.WINSIZE[0], self.WINSIZE[1], 2)))\n#         self.model.add(Activation('relu'))\n#         self.model.add(Conv2D(64, (8, 8), padding='same', strides=(4, 4)))\n#         self.model.add(Activation('relu'))\n#         self.model.add(Conv2D(128, (4, 4), padding='same', strides=(2, 2)))\n#         self.model.add(Activation('relu'))\n#         self.model.add(Conv2D(128, (2, 2), padding='same', strides=(1, 1)))\n#         self.model.add(Activation('relu'))\n#         self.model.add(Flatten())\n#         self.model.add(Dropout(0.5))\n#         self.model.add(Dense(128))\n#         self.model.add(Activation('relu'))\n#         self.model.add(Dropout(0.5))\n#         self.model.add(Dense(128))\n#         self.model.add(Dropout(0.5))\n#         self.model.add(Dense(1))\n#         self.model.compile(optimizer='adam', loss='mse')\n\n#     def load_weights(self):\n#         try:\n#             print(\"Loading weights...\")\n#             self.model.load_weights(self.WEIGHTS)\n#             return True\n#         except ValueError:\n#             print(\"Unable to load weights. Model has changed.\")\n#             print(\"Please retrain the model.\")\n#             return False\n#         except IOError:\n#             print(\"Unable to load weights. No previous weights found.\")\n#             print(\"Please train the model.\")\n#             return False\n\n#     def train(self, X_src, Y_src, val_split, wipe, n_epochs=50, batch_size=32):\n#         X, Y = self.prep_data(X_src, Y_src, shuffle=True, wipe=wipe)\n#         print(f\"TRAIN: Images - {X.shape}, Labels - {Y.shape}\")\n#         X = X[:, :, :, [0, 2]]  # Extract hue and value channels in data\n\n#         print(\"Training...\")\n#         self.model.fit(X, Y, batch_size=batch_size, epochs=n_epochs, validation_split=val_split)\n#         print(\"Done training. Saving weights...\")\n#         self.model.save_weights(self.WEIGHTS)\n\n#     def evaluate(self, X_src, Y_src):\n#         X_eval, Y_eval = self.prep_data(X_src, Y_src, shuffle=False)\n#         print(f\"EVAL: Images - {X_eval.shape}, Labels - {Y_eval.shape}\")\n\n#         success = self.load_weights()\n#         if success:\n#             print(\"Evaluating...\")\n#             print(self.model.evaluate(X_eval, Y_eval))\n#             print(\"Done evaluating\")\n#         else:\n#             print(\"Evaluation failed with improper weights.\")\n\n#     def play(self, X_src, Y_src, output_video_path=None):\n#         print(\"Reading Inputs...\")\n#         F = F = load_frames(\"train\")  \n#         X, Y = self.prep_data(X_src, Y_src, shuffle=False)\n\n#         if output_video_path:\n#             rec = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'MJPG'), 40, (640, 480), True)\n\n#         success = self.load_weights()\n#         if success:\n#             print(\"Playing video...\")\n#             for f, x, y in tqdm(zip(F, X, Y)):\n#                 frame = f\n#                 pred_y = self.model.predict(np.array([x[:, :, [0, 2]]]))[0, 0]\n#                 error = abs(y - pred_y)\n#                 font = cv2.FONT_HERSHEY_SIMPLEX\n#                 cv2.putText(frame, f'True: {y}', (30, 50), font, 1, (0, 255, 255), 2)\n#                 cv2.putText(frame, f'Pred: {pred_y}', (30, 150), font, 1, (0, 255, 255), 2)\n#                 cv2.putText(frame, f'Error: {error}', (30, 250), font, 1, (0, 255, 255), 2)\n#                 if output_video_path:\n#                     rec.write(frame)\n#             print(\"Done playing\")\n#             if output_video_path:\n#                 rec.release()\n#         else:\n#             print(\"Failed to load proper weights.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:06:59.005961Z","iopub.execute_input":"2024-06-16T12:06:59.006344Z","iopub.status.idle":"2024-06-16T12:06:59.017836Z","shell.execute_reply.started":"2024-06-16T12:06:59.006317Z","shell.execute_reply":"2024-06-16T12:06:59.016794Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nclass OptFlowNet:\n\n    def __init__(self, WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE):\n        self.WINSIZE = WINSIZE\n        self.WEIGHTS = WEIGHTS\n        self.EPOCHS = EPOCHS\n        self.BATCH_SIZE = BATCH_SIZE\n        self.optflow_dir = \"optical_flow_data\"  # Directory to save optical flow images\n\n    def process_frame(self, frame):\n        frame = cv2.resize(frame, (self.WINSIZE[1], self.WINSIZE[0]), interpolation=cv2.INTER_AREA)\n        frame = frame / 127.5 - 1.0  # standardize the image to mean = 0 and std = 1\n        return frame\n\n    def optflow(self, frame1, frame2):\n        frame1 = frame1[200:400]\n        frame1 = cv2.resize(frame1, (0, 0), fx=0.4, fy=0.5)\n        frame2 = frame2[200:400]\n        frame2 = cv2.resize(frame2, (0, 0), fx=0.4, fy=0.5)\n        flow = np.zeros_like(frame1)\n        prev = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n        nxt = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n        flow_data = cv2.calcOpticalFlowFarneback(prev, nxt, None, 0.4, 1, 12, 2, 8, 1.2, 0)\n        mag, ang = cv2.cartToPolar(flow_data[..., 0], flow_data[..., 1])\n        flow[..., 1] = 255  # saturation\n        flow[..., 0] = ang * 180 / np.pi / 2  # hue\n        flow[..., 2] = (mag * 15).astype(int)  # value\n        return flow\n\n    def prep_data(self, video_file, label_file, shuffle=False, wipe=False, mode=\"train\"):\n        print(\"Decoding Labels...\")\n        if mode != \"test\":\n            speed_data = np.array(pd.read_csv(label_file, header=None))\n            speed_data = speed_data.flatten()[:-1]  # assuming the last entry is redundant\n            print(f\"Loaded {len(speed_data)} labels\")\n        else:\n            shuffle = False\n            speed_data = np.array([])\n\n        if wipe and os.path.isdir(self.optflow_dir):\n            print(\"Wiping preprocessed data...\")\n            shutil.rmtree(self.optflow_dir)\n\n        processed_video = None\n        if not os.path.isdir(self.optflow_dir):\n            print(\"Preprocessing video...\")\n            os.mkdir(self.optflow_dir)\n            vid = cv2.VideoCapture(video_file)\n            frame_cnt = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n            processed_video = np.empty((frame_cnt - 1, self.WINSIZE[0], self.WINSIZE[1], 3), dtype='uint8')\n            success, prev = vid.read()\n            i = 0\n            while True:\n                success, nxt = vid.read()\n                if not success:\n                    break\n                flow = self.optflow(prev, nxt)\n                prev = nxt\n                flow = cv2.resize(flow, self.WINSIZE, interpolation=cv2.INTER_AREA)\n                processed_video[i] = flow / 127.5 - 1.0\n                cv2.imwrite(f\"{self.optflow_dir}/{i}.png\", flow)\n                sys.stdout.write(f\"\\rProcessed {i} frames\")\n                i += 1\n            print(f\"\\nDone processing {frame_cnt} frames\")\n        else:\n            print(\"Found preprocessed data\")\n            frame_cnt = len(os.listdir(self.optflow_dir))\n            processed_video = np.empty((frame_cnt, self.WINSIZE[0], self.WINSIZE[1], 3), dtype='float32')\n            for i in range(frame_cnt):\n                flow = cv2.imread(f\"{self.optflow_dir}/{i}.png\")\n                flow = self.process_frame(flow)\n                processed_video[i] = flow\n                sys.stdout.write(f\"\\rLoading frame {i}\")\n            print(f\"\\nDone loading {frame_cnt} frames\")\n\n        if shuffle:\n            print(\"Shuffling data\")\n            randomize = np.arange(len(processed_video))\n            np.random.shuffle(randomize)\n            processed_video = processed_video[randomize]\n            speed_data = speed_data[randomize]\n\n        print(\"Done prepping data\")\n        return processed_video, speed_data\n\n    def create_model(self):\n        print(\"Compiling Model...\")\n        self.model = Sequential()\n        self.model.add(Conv2D(32, (8, 8), padding='same', strides=(4, 4), input_shape=(self.WINSIZE[0], self.WINSIZE[1], 2)))\n        self.model.add(Activation('relu'))\n        self.model.add(Conv2D(64, (8, 8), padding='same', strides=(4, 4)))\n        self.model.add(Activation('relu'))\n        self.model.add(Conv2D(128, (4, 4), padding='same', strides=(2, 2)))\n        self.model.add(Activation('relu'))\n        self.model.add(Conv2D(128, (2, 2), padding='same', strides=(1, 1)))\n        self.model.add(Activation('relu'))\n        self.model.add(Flatten())\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(128))\n        self.model.add(Activation('relu'))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(128))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(1))\n        self.model.compile(optimizer='adam', loss='mse')\n\n    def load_frames(self, video_file):\n        vid = cv2.VideoCapture(video_file)\n        frames = []\n        while True:\n            success, frame = vid.read()\n            if not success:\n                break\n            frames.append(frame)\n        vid.release()\n        return frames\n    \n    def load_weights(self):\n        try:\n            print(\"Loading weights...\")\n            self.model.load_weights(self.WEIGHTS)\n            return True\n        except ValueError:\n            print(\"Unable to load weights. Model has changed.\")\n            print(\"Please retrain the model.\")\n            return False\n        except IOError:\n            print(\"Unable to load weights. No previous weights found.\")\n            print(\"Please train the model.\")\n            return False\n\n    def train(self, X_src, Y_src, val_split, wipe, n_epochs=50, batch_size=32):\n        X, Y = self.prep_data(X_src, Y_src, shuffle=True, wipe=wipe)\n        print(f\"TRAIN: Images - {X.shape}, Labels - {Y.shape}\")\n        X = X[:, :, :, [0, 2]]  # Extract hue and value channels in data\n\n        print(\"Training...\")\n        self.model.fit(X, Y, batch_size=batch_size, epochs=n_epochs, validation_split=val_split, verbose=0)  # Set verbose=0 to suppress output\n        print(\"Done training. Saving weights...\")\n        self.model.save_weights(self.WEIGHTS)\n\n    def evaluate(self, X_src, Y_src):\n        X_eval, Y_eval = self.prep_data(X_src, Y_src, shuffle=False)\n        print(f\"EVAL: Images - {X_eval.shape}, Labels - {Y_eval.shape}\")\n\n        success = self.load_weights()\n        if success:\n            print(\"Evaluating...\")\n            self.model.evaluate(X_eval, Y_eval, verbose=0)  # Set verbose=0 to suppress output\n            print(\"Done evaluating\")\n        else:\n            print(\"Evaluation failed with improper weights.\")\n\n    def play(self, X_src, Y_src, output_video_path=None):\n        print(\"Reading Inputs...\")\n        F = self.load_frames(X_src)\n        X, Y = self.prep_data(X_src, Y_src, shuffle=False)\n\n        if output_video_path:\n            fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n            frame_height, frame_width = F[0].shape[:2]\n            rec = cv2.VideoWriter(output_video_path, fourcc, 40, (frame_width, frame_height), True)\n\n        success = self.load_weights()\n        if success:\n            print(\"Playing video...\")\n            # Disable all output\n            original_stdout = sys.stdout\n            sys.stdout = open(os.devnull, 'w')\n\n            for f, x, y in zip(F, X, Y):\n                frame = f\n                pred_y = self.model.predict(np.array([x[:, :, [0, 2]]]))[0, 0]\n                error = abs(y - pred_y)\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(frame, f'True: {y}', (30, 50), font, 1, (0, 255, 255), 2)\n                cv2.putText(frame, f'Pred: {pred_y}', (30, 150), font, 1, (0, 255, 255), 2)\n                cv2.putText(frame, f'Error: {error}', (30, 250), font, 1, (0, 255, 255), 2)\n                if output_video_path:\n                    rec.write(frame)\n\n            # Revert stdout back to normal\n            sys.stdout = original_stdout\n\n            print(\"Done playing\")\n            if output_video_path:\n                rec.release()\n        else:\n            print(\"Failed to load proper weights.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:07:03.489825Z","iopub.execute_input":"2024-06-16T12:07:03.490200Z","iopub.status.idle":"2024-06-16T12:07:03.532940Z","shell.execute_reply.started":"2024-06-16T12:07:03.490175Z","shell.execute_reply":"2024-06-16T12:07:03.531480Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    WINSIZE = (100, 100)\n    WEIGHTS = \".weights.h5\"\n    EPOCHS = 30\n    BATCH_SIZE = 32\n    \n    video_file = \"/kaggle/input/q3-computer-vision/train.mp4\"\n    label_file = \"/kaggle/input/q3-computer-vision/train.txt\"\n    \n    model = OptFlowNet(WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE)\n\n    # Choose mode\n    mode = \"train\"  # Change this to \"evaluate\" or \"play\" as needed\n    val_split = 0.3\n    resume = True\n    wipe = True\n\n    # Create the model\n    model.create_model()\n\n    # Load weights if resuming\n    if resume:\n        model.load_weights()\n\n    # Train or evaluate based on the mode\n    if mode == \"train\":\n        model.train(video_file, label_file, val_split, wipe, n_epochs=EPOCHS, batch_size=BATCH_SIZE)\n    elif mode == \"evaluate\":\n        model.evaluate(video_file, label_file)\n    elif mode == \"play\":\n        model.play(video_file, label_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:09:14.681021Z","iopub.execute_input":"2024-06-16T12:09:14.681792Z","iopub.status.idle":"2024-06-16T12:13:46.436224Z","shell.execute_reply.started":"2024-06-16T12:09:14.681761Z","shell.execute_reply":"2024-06-16T12:13:46.435161Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Compiling Model...\nLoading weights...\nUnable to load weights. No previous weights found.\nPlease train the model.\nDecoding Labels...\nLoaded 20399 labels\nWiping preprocessed data...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Preprocessing video...\nProcessed 20398 frames\nDone processing 20400 frames\nShuffling data\nDone prepping data\nTRAIN: Images - (20399, 100, 100, 3), Labels - (20399,)\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718539948.114884      86 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1718539948.136956      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718539958.526065      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718539959.901341      86 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718539961.120657      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Done training. Saving weights...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"qweaddghg","metadata":{}},{"cell_type":"code","source":"print('here')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:13:46.438176Z","iopub.execute_input":"2024-06-16T12:13:46.438470Z","iopub.status.idle":"2024-06-16T12:13:46.443575Z","shell.execute_reply.started":"2024-06-16T12:13:46.438447Z","shell.execute_reply":"2024-06-16T12:13:46.442370Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"here\n","output_type":"stream"}]},{"cell_type":"code","source":"model = OptFlowNet(WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE)\nmode = \"play\" \nval_split = 0.3\nresume = True\nwipe = True\n\n\n# Create the model\nmodel.create_model()\n\nif resume:\n    model.load_weights()\n\n# Train or evaluate based on the mode\nif mode == \"train\":\n    model.train(video_file, label_file, val_split, wipe, n_epochs=EPOCHS, batch_size=BATCH_SIZE)\nelif mode == \"evaluate\":\n    model.evaluate(video_file, label_file)\nelif mode == \"play\":\n    output_video_path = \"/kaggle/working/output_video.avi\"\n    model.play(video_file, label_file, output_video_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:15:06.209508Z","iopub.execute_input":"2024-06-16T12:15:06.209917Z","iopub.status.idle":"2024-06-16T12:25:28.443652Z","shell.execute_reply.started":"2024-06-16T12:15:06.209890Z","shell.execute_reply":"2024-06-16T12:25:28.442355Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Compiling Model...\nLoading weights...\nReading Inputs...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Decoding Labels...\nLoaded 10797 labels\nFound preprocessed data\nLoading frame 20398\nDone loading 20399 frames\nDone prepping data\nLoading weights...\nPlaying video...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 30 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     19\u001b[0m     output_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/output_video.avi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_video_path\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 173\u001b[0m, in \u001b[0;36mOptFlowNet.play\u001b[0;34m(self, X_src, Y_src, output_video_path)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f, x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(F, X, Y):\n\u001b[1;32m    172\u001b[0m     frame \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 173\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    174\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(y \u001b[38;5;241m-\u001b[39m pred_y)\n\u001b[1;32m    175\u001b[0m     font \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:506\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    504\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function(data)\n\u001b[1;32m    505\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[0;32m--> 506\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_predicting:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:126\u001b[0m, in \u001b[0;36mCallbackList.on_predict_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    124\u001b[0m logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/progbar_logger.py:66\u001b[0m, in \u001b[0;36mProgbarLogger.on_predict_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_predict_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Don't pass prediction results.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/progbar_logger.py:95\u001b[0m, in \u001b[0;36mProgbarLogger._update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# One-indexed.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/progbar.py:182\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finalize:\n\u001b[1;32m    180\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 182\u001b[0m \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_total_width \u001b[38;5;241m=\u001b[39m total_width\n\u001b[1;32m    184\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/io_utils.py:99\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[0;32m---> 99\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print('done')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T12:25:28.444744Z","iopub.status.idle":"2024-06-16T12:25:28.445245Z","shell.execute_reply.started":"2024-06-16T12:25:28.444969Z","shell.execute_reply":"2024-06-16T12:25:28.444989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Use the correct relative path\nfile_path = 'output_video.avi'  \nFileLink(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T10:46:48.754677Z","iopub.execute_input":"2024-06-16T10:46:48.755018Z","iopub.status.idle":"2024-06-16T10:46:48.761419Z","shell.execute_reply.started":"2024-06-16T10:46:48.754994Z","shell.execute_reply":"2024-06-16T10:46:48.760515Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output_video.avi","text/html":"<a href='output_video.avi' target='_blank'>output_video.avi</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"# part 2","metadata":{}},{"cell_type":"code","source":"\nclass OptFlowNet:\n\n    def __init__(self, WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE):\n        self.WINSIZE = WINSIZE\n        self.WEIGHTS = WEIGHTS\n        self.EPOCHS = EPOCHS\n        self.BATCH_SIZE = BATCH_SIZE\n        self.optflow_dir = \"optical_flow_data\"  # Directory to save optical flow images\n\n    def process_frame(self, frame):\n        frame = cv2.resize(frame, (self.WINSIZE[1], self.WINSIZE[0]), interpolation=cv2.INTER_AREA)\n        frame = frame / 127.5 - 1.0  # standardize the image to mean = 0 and std = 1\n        return frame\n\n    def optflow(self, frame1, frame2):\n        frame1 = frame1[200:400]\n        frame1 = cv2.resize(frame1, (0, 0), fx=0.4, fy=0.5)\n        frame2 = frame2[200:400]\n        frame2 = cv2.resize(frame2, (0, 0), fx=0.4, fy=0.5)\n        flow = np.zeros_like(frame1)\n        prev = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n        nxt = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n        flow_data = cv2.calcOpticalFlowFarneback(prev, nxt, None, 0.4, 1, 12, 2, 8, 1.2, 0)\n        mag, ang = cv2.cartToPolar(flow_data[..., 0], flow_data[..., 1])\n        flow[..., 1] = 255  # saturation\n        flow[..., 0] = ang * 180 / np.pi / 2  # hue\n        flow[..., 2] = (mag * 15).astype(int)  # value\n        return flow\n\n    def prep_data(self, video_file, label_file, shuffle=False, wipe=False, mode=\"train\"):\n        print(\"Decoding Labels...\")\n        if mode != \"test\":\n            speed_data = np.array(pd.read_csv(label_file, header=None))\n            speed_data = speed_data.flatten()[:-1]  # assuming the last entry is redundant\n            print(f\"Loaded {len(speed_data)} labels\")\n        else:\n            shuffle = False\n            speed_data = np.array([])\n\n        if wipe and os.path.isdir(self.optflow_dir):\n            print(\"Wiping preprocessed data...\")\n            shutil.rmtree(self.optflow_dir)\n\n        processed_video = None\n        if not os.path.isdir(self.optflow_dir):\n            print(\"Preprocessing video...\")\n            os.mkdir(self.optflow_dir)\n            vid = cv2.VideoCapture(video_file)\n            frame_cnt = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n            processed_video = np.empty((frame_cnt - 1, self.WINSIZE[0], self.WINSIZE[1], 3), dtype='uint8')\n            success, prev = vid.read()\n            i = 0\n            while True:\n                success, nxt = vid.read()\n                if not success:\n                    break\n                flow = self.optflow(prev, nxt)\n                prev = nxt\n                flow = cv2.resize(flow, self.WINSIZE, interpolation=cv2.INTER_AREA)\n                processed_video[i] = flow / 127.5 - 1.0\n                cv2.imwrite(f\"{self.optflow_dir}/{i}.png\", flow)\n                sys.stdout.write(f\"\\rProcessed {i} frames\")\n                i += 1\n            print(f\"\\nDone processing {frame_cnt} frames\")\n        else:\n            print(\"Found preprocessed data\")\n            frame_cnt = len(os.listdir(self.optflow_dir))\n            processed_video = np.empty((frame_cnt, self.WINSIZE[0], self.WINSIZE[1], 3), dtype='float32')\n            for i in range(frame_cnt):\n                flow = cv2.imread(f\"{self.optflow_dir}/{i}.png\")\n                flow = self.process_frame(flow)\n                processed_video[i] = flow\n                sys.stdout.write(f\"\\rLoading frame {i}\")\n            print(f\"\\nDone loading {frame_cnt} frames\")\n\n        if shuffle:\n            print(\"Shuffling data\")\n            randomize = np.arange(len(processed_video))\n            np.random.shuffle(randomize)\n            processed_video = processed_video[randomize]\n            speed_data = speed_data[randomize]\n\n        print(\"Done prepping data\")\n        return processed_video, speed_data\n\n    def create_model(self):\n        print(\"Compiling Model...\")\n        self.model = Sequential()\n        self.model.add(Conv2D(32, (8, 8), padding='same', strides=(4, 4), input_shape=(self.WINSIZE[0], self.WINSIZE[1], 2)))\n        self.model.add(Activation('relu'))\n        self.model.add(Conv2D(64, (8, 8), padding='same', strides=(4, 4)))\n        self.model.add(Activation('relu'))\n        self.model.add(Conv2D(128, (4, 4), padding='same', strides=(2, 2)))\n        self.model.add(Activation('relu'))\n        self.model.add(Conv2D(128, (2, 2), padding='same', strides=(1, 1)))\n        self.model.add(Activation('relu'))\n        self.model.add(Flatten())\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(128))\n        self.model.add(Activation('relu'))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(128))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(1))\n        self.model.compile(optimizer='adam', loss='mse')\n\n    def load_frames(self, video_file):\n        vid = cv2.VideoCapture(video_file)\n        frames = []\n        while True:\n            success, frame = vid.read()\n            if not success:\n                break\n            frames.append(frame)\n        vid.release()\n        return frames\n    \n    def load_weights(self):\n        try:\n            print(\"Loading weights...\")\n            self.model.load_weights(self.WEIGHTS)\n            return True\n        except ValueError:\n            print(\"Unable to load weights. Model has changed.\")\n            print(\"Please retrain the model.\")\n            return False\n        except IOError:\n            print(\"Unable to load weights. No previous weights found.\")\n            print(\"Please train the model.\")\n            return False\n\n    def train(self, X_src, Y_src, val_split, wipe, n_epochs=50, batch_size=32):\n        X, Y = self.prep_data(X_src, Y_src, shuffle=True, wipe=wipe)\n        print(f\"TRAIN: Images - {X.shape}, Labels - {Y.shape}\")\n        X = X[:, :, :, [0, 2]]  # Extract hue and value channels in data\n\n        print(\"Training...\")\n        self.model.fit(X, Y, batch_size=batch_size, epochs=n_epochs, validation_split=val_split, verbose=0)  # Set verbose=0 to suppress output\n        print(\"Done training. Saving weights...\")\n        self.model.save_weights(self.WEIGHTS)\n\n    def evaluate(self, X_src, Y_src, output_video_path=None):\n        X_eval, Y_eval = self.prep_data(X_src, Y_src, shuffle=False)\n        print(f\"EVAL: Images - {X_eval.shape}, Labels - {Y_eval.shape}\")\n\n        success = self.load_weights()\n        if success:\n            print(\"Evaluating...\")\n            if output_video_path:\n                fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n                frame_height, frame_width = X_eval[0].shape[:2]\n                rec = cv2.VideoWriter(output_video_path, fourcc, 40, (frame_width, frame_height), True)\n\n            for x, y in zip(X_eval, Y_eval):\n                pred_y = self.model.predict(np.array([x[:, :, [0, 2]]]))[0, 0]\n                error = abs(y - pred_y)\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(x, f'True: {y}', (30, 50), font, 1, (0, 255, 255), 2)\n                cv2.putText(x, f'Pred: {pred_y}', (30, 150), font, 1, (0, 255, 255), 2)\n                cv2.putText(x, f'Error: {error}', (30, 250), font, 1, (0, 255, 255), 2)\n                if output_video_path:\n                    rec.write(x)\n\n            if output_video_path:\n                rec.release()\n            print(\"Done evaluating\")\n        else:\n            print(\"Evaluation failed with improper weights.\")\n\n    def play(self, X_src, Y_src, output_video_path=None):\n        print(\"Reading Inputs...\")\n        F = self.load_frames(X_src)\n        X, Y = self.prep_data(X_src, Y_src, shuffle=False)\n\n        if output_video_path:\n            fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n            frame_height, frame_width = F[0].shape[:2]\n            rec = cv2.VideoWriter(output_video_path, fourcc, 40, (frame_width, frame_height), True)\n\n        success = self.load_weights()\n        if success:\n            print(\"Playing video...\")\n            # Disable all output\n            original_stdout = sys.stdout\n            sys.stdout = open(os.devnull, 'w')\n\n            for f, x, y in zip(F, X, Y):\n                frame = f\n                pred_y = self.model.predict(np.array([x[:, :, [0, 2]]]))[0, 0]\n                error = abs(y - pred_y)\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(frame, f'True: {y}', (30, 50), font, 1, (0, 255, 255), 2)\n                cv2.putText(frame, f'Pred: {pred_y}', (30, 150), font, 1, (0, 255, 255), 2)\n                cv2.putText(frame, f'Error: {error}', (30, 250), font, 1, (0, 255, 255), 2)\n                if output_video_path:\n                    rec.write(frame)\n\n            # Revert stdout back to normal\n            sys.stdout = original_stdout\n\n            print(\"Done playing\")\n            if output_video_path:\n                rec.release()\n        else:\n            print(\"Failed to load proper weights.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T19:41:44.325344Z","iopub.execute_input":"2024-06-16T19:41:44.325944Z","iopub.status.idle":"2024-06-16T19:41:44.376585Z","shell.execute_reply.started":"2024-06-16T19:41:44.325904Z","shell.execute_reply":"2024-06-16T19:41:44.375703Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    WINSIZE = (100, 100)\n    WEIGHTS = \".weights.h5\"\n    EPOCHS = 30\n    BATCH_SIZE = 32\n    \n    video_file = \"/kaggle/input/q3-computer-vision/train.mp4\"\n    label_file = \"/kaggle/input/q3-computer-vision/train.txt\"\n    \n    model = OptFlowNet(WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE)\n\n    # Choose mode\n    mode = \"train\"  # Change this to \"evaluate\" or \"play\" as needed\n    val_split = 0.3\n    resume = True\n    wipe = True\n\n    # Create the model\n    model.create_model()\n\n    # Load weights if resuming\n    if resume:\n        model.load_weights()\n\n    # Train or evaluate based on the mode\n    if mode == \"train\":\n        model.train(video_file, label_file, val_split, wipe, n_epochs=EPOCHS, batch_size=BATCH_SIZE)\n    elif mode == \"evaluate\":\n        model.evaluate(video_file, label_file)\n    elif mode == \"play\":\n        model.play(video_file, label_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T19:41:44.377869Z","iopub.execute_input":"2024-06-16T19:41:44.378233Z","iopub.status.idle":"2024-06-16T19:46:08.870766Z","shell.execute_reply.started":"2024-06-16T19:41:44.378207Z","shell.execute_reply":"2024-06-16T19:46:08.869856Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Compiling Model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Loading weights...\nUnable to load weights. No previous weights found.\nPlease train the model.\nDecoding Labels...\nLoaded 20399 labels\nPreprocessing video...\nProcessed 20398 frames\nDone processing 20400 frames\nShuffling data\nDone prepping data\nTRAIN: Images - (20399, 100, 100, 3), Labels - (20399,)\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1718567092.813813      87 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1718567092.835721      87 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718567102.450069      89 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718567103.807350      88 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718567105.033489      89 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Done training. Saving weights...\n","output_type":"stream"}]},{"cell_type":"code","source":"print('1')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T19:46:52.877823Z","iopub.execute_input":"2024-06-16T19:46:52.878588Z","iopub.status.idle":"2024-06-16T19:46:52.883749Z","shell.execute_reply.started":"2024-06-16T19:46:52.878559Z","shell.execute_reply":"2024-06-16T19:46:52.882513Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"model = OptFlowNet(WINSIZE, WEIGHTS, EPOCHS, BATCH_SIZE)\nmode = \"play\" \nval_split = 0.3\nresume = True\nwipe = True\n\nvideo_file = \"/kaggle/input/q3-computer-vision/test.mp4\"\nlabel_file = \"/kaggle/input/q3-computer-vision/test.txt\"\n\n\nmodel.create_model()\n\nif resume:\n    model.load_weights()\n\n# Train or evaluate based on the mode\nif mode == \"train\":\n    model.train(video_file, label_file, val_split, wipe, n_epochs=EPOCHS, batch_size=BATCH_SIZE)\nelif mode == \"evaluate\":\n    model.evaluate(video_file, label_file)\nelif mode == \"play\":\n    output_video_path = \"/kaggle/working/output_video_with_optical_flow.avi\"\n    model.play(video_file, label_file, output_video_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T19:46:55.738519Z","iopub.execute_input":"2024-06-16T19:46:55.739206Z","iopub.status.idle":"2024-06-16T19:58:02.366540Z","shell.execute_reply.started":"2024-06-16T19:46:55.739173Z","shell.execute_reply":"2024-06-16T19:58:02.365575Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Compiling Model...\nLoading weights...\nReading Inputs...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 30 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"Decoding Labels...\nLoaded 10797 labels\nFound preprocessed data\nLoading frame 20398\nDone loading 20399 frames\nDone prepping data\nLoading weights...\nPlaying video...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 30 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"Done playing\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Use the correct relative path\nfile_path = 'output_video_with_optical_flow.avi'  \nFileLink(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T20:03:22.314753Z","iopub.execute_input":"2024-06-16T20:03:22.315590Z","iopub.status.idle":"2024-06-16T20:03:22.322673Z","shell.execute_reply.started":"2024-06-16T20:03:22.315554Z","shell.execute_reply":"2024-06-16T20:03:22.321843Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output_video_with_optical_flow.avi","text/html":"<a href='output_video_with_optical_flow.avi' target='_blank'>output_video_with_optical_flow.avi</a><br>"},"metadata":{}}]}]}